# è®­ç»ƒé…ç½®æŒ‡å— (Training Configuration Guide)

## ğŸ¯ å¿«é€Ÿå¼€å§‹

è¿è¡Œè®­ç»ƒï¼š
```bash
python HachimiNetV1/main.py
```

## ğŸ“‹ GPU/CPU è®¾ç½®

### é—®é¢˜ï¼šä¸ºä»€ä¹ˆè®­ç»ƒåœ¨CPUä¸Šï¼Ÿ
ä½ çš„ç³»ç»Ÿæ²¡æœ‰æ£€æµ‹åˆ°GPUã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š
1. **æ²¡æœ‰å®‰è£…NVIDIA GPU**
2. **æ²¡æœ‰å®‰è£…CUDA** - éœ€è¦ CUDA 12.1+ å’Œ cuDNN 
3. **PyTorchæœªé…ç½®CUDAæ”¯æŒ** - éœ€è¦å®‰è£… `torch[cuda]`

### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šå¼ºåˆ¶ä½¿ç”¨GPUï¼ˆå¦‚æœæœ‰ç¡¬ä»¶ï¼‰**

ç¼–è¾‘ `HachimiNetV1/configs/train_config.yaml`ï¼š
```yaml
training:
  device: "cuda"  # æ”¹ä¸º "cuda" å¼ºåˆ¶ä½¿ç”¨GPU
  # æˆ–ä¿æŒ "auto" è®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©
```

**æ–¹æ¡ˆ2ï¼šæ£€æŸ¥å’Œå®‰è£…CUDAæ”¯æŒ**

```bash
# æ£€æŸ¥å½“å‰PyTorchæ˜¯å¦æ”¯æŒCUDA
python -c "import torch; print(torch.cuda.is_available())"

# å¦‚æœä¸ºFalseï¼Œé‡æ–°å®‰è£…æ”¯æŒCUDAçš„PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## âš™ï¸ è®­ç»ƒè¶…å‚æ•°è°ƒèŠ‚

æ‰€æœ‰è®­ç»ƒå‚æ•°éƒ½åœ¨ `HachimiNetV1/configs/train_config.yaml` ä¸­çš„ `training` éƒ¨åˆ†ï¼š

```yaml
training:
  # ===== GPU è®¾ç½® =====
  device: "auto"      # "auto" (è‡ªåŠ¨), "cuda" (å¼ºåˆ¶GPU), "cpu" (å¼ºåˆ¶CPU)
  
  # ===== è®­ç»ƒåŸºç¡€å‚æ•° =====
  num_epochs: 50              # è®­ç»ƒè½®æ•°ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†æ—¶é—´é•¿ï¼‰
  batch_size: 4               # æ‰¹æ¬¡å¤§å°ï¼ˆGPUå†…å­˜å……è¶³å¯å¢åŠ åˆ°8,16,32ï¼‰
  learning_rate: 1.0e-4       # å­¦ä¹ ç‡ï¼ˆé€šå¸¸åœ¨1e-3åˆ°1e-5ä¹‹é—´ï¼‰
  weight_decay: 1.0e-5        # L2æ­£åˆ™åŒ–å¼ºåº¦
  
  # ===== ä¼˜åŒ–å™¨è®¾ç½® =====
  optimizer: "AdamW"          # "AdamW" æˆ– "Adam"
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  
  # ===== æ¢¯åº¦å¤„ç† =====
  gradient_clip_norm: 1.0     # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
  
  # ===== Loss å‡½æ•°æƒé‡ =====
  regression_loss_weight: 1.0       # MSE Lossæƒé‡
  cosine_loss_weight: 0.5           # Cosineç›¸ä¼¼åº¦Lossæƒé‡
  
  # ===== å…¶ä»– =====
  num_workers: 0              # DataLoaderè¿›ç¨‹æ•°ï¼ˆ0=ä¸»è¿›ç¨‹ï¼Œ>0=å¤šè¿›ç¨‹ï¼‰
  lazy_load: true             # æ˜¯å¦å¯ç”¨lazy loadingï¼ˆæ¨ètrueèŠ‚çœå†…å­˜ï¼‰
  checkpoint_interval: 5      # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
```

## ğŸ“Š å¸¸è§è°ƒå‚å»ºè®®

### åœºæ™¯1ï¼šå†…å­˜ä¸è¶³æˆ–æ˜¾å­˜ä¸è¶³
```yaml
batch_size: 2          # å‡å°æ‰¹æ¬¡å¤§å°
num_workers: 0         # å…³é—­å¤šè¿›ç¨‹
lazy_load: true        # å¯ç”¨lazy loading
```

### åœºæ™¯2ï¼šæƒ³è¦æ›´å¥½çš„ç²¾åº¦ï¼ˆéœ€è¦æ›´å¤šæ—¶é—´å’Œå†…å­˜ï¼‰
```yaml
num_epochs: 100        # å¢åŠ è®­ç»ƒè½®æ•°
batch_size: 16         # å¢å¤§æ‰¹æ¬¡ï¼ˆå¦‚æœæ˜¾å­˜è¶³å¤Ÿï¼‰
learning_rate: 5.0e-5  # é™ä½å­¦ä¹ ç‡ï¼Œè®­ç»ƒæ›´ç¨³å®š
```

### åœºæ™¯3ï¼šæƒ³è¦æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦
```yaml
num_epochs: 20         # å‡å°‘è®­ç»ƒè½®æ•°
batch_size: 8          # æ‰¾å¹³è¡¡ç‚¹
learning_rate: 1.0e-4  # ä¿æŒè¾ƒé«˜
num_workers: 4         # å¦‚æœCPUè¶³å¤Ÿï¼Œå¢åŠ DataLoaderè¿›ç¨‹
```

### åœºæ™¯4ï¼šGPUè®­ç»ƒï¼ˆæ¨èï¼‰
```yaml
device: "cuda"
batch_size: 16
learning_rate: 1.0e-4
num_workers: 4         # GPUæ—¶å¯ä»¥ç”¨å¤šè¿›ç¨‹
```

## ğŸ”§ ä»£ç ä¸­çš„å…¶ä»–å¯è°ƒå‚æ•°

### main.py ä¸­çš„å‚æ•°

é™¤äº†é…ç½®æ–‡ä»¶ï¼Œè¿˜å¯ä»¥åœ¨ `main.py` ä¸­ç›´æ¥ä¿®æ”¹ï¼š
```python
train_cfg = cfg.get('training', {})
num_epochs = train_cfg.get('num_epochs', 50)     # é»˜è®¤å€¼
batch_size = train_cfg.get('batch_size', 4)       # é»˜è®¤å€¼
lazy_load = train_cfg.get('lazy_load', True)      # é»˜è®¤å€¼
```

### models/loss.py ä¸­çš„Lossæƒé‡

```python
# å›å½’Lossæƒé‡è°ƒèŠ‚
loss_fn = WeightsRegressionLoss(
    lambda_cos=0.5  # CosineæŸå¤±çš„æƒé‡ï¼ˆ0=ä»…MSEï¼Œ1=å¹³è¡¡ï¼‰
)
```

## ğŸ“ˆ è®­ç»ƒç›‘æ§

è®­ç»ƒæ—¶ä¼šæ‰“å°ï¼š
```
=====================================================================
ğŸ“Š Training Config
=====================================================================
Epochs: 50
Batch Size: 4
Learning Rate: 0.0001
Lazy Load: True
=====================================================================

ğŸ“Œ Device: cuda:0
ğŸ“Œ CUDA Available: True
   GPU: NVIDIA RTX 3090
   VRAM: 24.0 GB

ğŸš€ Training started on cuda:0
   Model Parameters: 123,456
   Trainable Parameters: 100,000
   
Epoch 1/50 | Train Loss: 0.5234 | Val Loss: 0.4892
Epoch 2/50 | Train Loss: 0.4856 | Val Loss: 0.4523
...
```

## ğŸ’¾ Checkpointä¿å­˜

æ¨¡å‹ä¼šæ¯5ä¸ªepochï¼ˆå¯é…ç½®ï¼‰ä¿å­˜åˆ° `./checkpoints/` ç›®å½•ï¼š
```
./checkpoints/
â”œâ”€â”€ model_ep5.pth
â”œâ”€â”€ model_ep10.pth
â”œâ”€â”€ model_ep15.pth
â””â”€â”€ ...
```

## ğŸ› å¸¸è§é—®é¢˜

**Q: è®­ç»ƒå¾ˆæ…¢ï¼Ÿ**
- A: æ£€æŸ¥æ˜¯å¦åœ¨CPUä¸Šè®­ç»ƒã€‚å¦‚æœæœ‰GPUï¼Œç¡®ä¿ `device: "cuda"`

**Q: æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰ï¼Ÿ**
- A: å‡å° `batch_size` (4 â†’ 2) æˆ–å¯ç”¨ `lazy_load: true`

**Q: Lossä¸ä¸‹é™ï¼Ÿ**
- A: å°è¯•é™ä½ `learning_rate` (1e-4 â†’ 5e-5 æˆ–æ›´ä½)

**Q: æƒ³è¦æ–­ç‚¹ç»­è®­ï¼Ÿ**
- A: éœ€è¦åŠ è½½checkpointï¼Œç›®å‰ä»£ç æœªå®ç°ï¼Œå¯è‡ªè¡Œæ·»åŠ 

## ğŸ“ å®Œæ•´é…ç½®ç¤ºä¾‹

**GPUé«˜æ€§èƒ½è®­ç»ƒï¼š**
```yaml
training:
  device: "cuda"
  num_epochs: 100
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  optimizer: "AdamW"
  gradient_clip_norm: 1.0
  cosine_loss_weight: 0.5
  num_workers: 4
  lazy_load: true
  checkpoint_interval: 5
```

**CPUä½é…è®­ç»ƒï¼š**
```yaml
training:
  device: "cpu"
  num_epochs: 20
  batch_size: 2
  learning_rate: 5.0e-5
  weight_decay: 1.0e-5
  optimizer: "AdamW"
  gradient_clip_norm: 1.0
  cosine_loss_weight: 0.5
  num_workers: 0
  lazy_load: true
  checkpoint_interval: 10
```
