# HachimiNetV1 è®­ç»ƒå¿«é€Ÿå‚è€ƒ

## ğŸ“Œ å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆè®­ç»ƒåœ¨CPUä¸Šï¼Œä¸åœ¨GPUä¸Šï¼Ÿ
**åŸå› ï¼š** ä½ çš„ç³»ç»Ÿæ²¡æœ‰å¯ç”¨çš„GPUï¼Œæˆ–PyTorchæœªé…ç½®CUDAæ”¯æŒ

**è§£å†³æ–¹æ¡ˆï¼š**
1. **æ£€æŸ¥GPU**: è¿è¡Œ `python -c "import torch; print(torch.cuda.is_available())"`
2. **å¦‚æœæ˜¯False**ï¼Œéœ€è¦å®‰è£…CUDAæ”¯æŒçš„PyTorchï¼š
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```
3. **æˆ–ä¿®æ”¹é…ç½®å¼ºåˆ¶CPU** (å¦‚æœæ²¡æœ‰GPU)ï¼š
   ç¼–è¾‘ `HachimiNetV1/configs/train_config.yaml`
   ```yaml
   training:
     device: "cpu"  # æ”¹ä¸ºcpu
   ```

---

## âš™ï¸ è®­ç»ƒå‚æ•°åœ¨å“ªé‡Œè°ƒèŠ‚ï¼Ÿ

### ä¸»é…ç½®æ–‡ä»¶ï¼š`HachimiNetV1/configs/train_config.yaml`

**GPU/Deviceè®¾ç½®ï¼š**
```yaml
training:
  device: "auto"  # "auto"(è‡ªåŠ¨), "cuda"(å¼ºåˆ¶GPU), "cpu"(å¼ºåˆ¶CPU)
```

**å…³é”®è®­ç»ƒå‚æ•°ï¼š**
```yaml
training:
  num_epochs: 50                  # è®­ç»ƒè½®æ•°
  batch_size: 4                   # æ‰¹å¤§å° (â†‘å†…å­˜ä½¿ç”¨, â†“æ”¶æ•›ç¨³å®šæ€§)
  learning_rate: 1.0e-4           # å­¦ä¹ ç‡ (â†‘æ”¶æ•›å¿«ä½†å¯èƒ½æŒ¯è¡, â†“æ”¶æ•›æ…¢ä½†ç¨³å®š)
  weight_decay: 1.0e-5            # L2æ­£åˆ™åŒ–
  optimizer: "AdamW"              # ä¼˜åŒ–å™¨ç±»å‹
  gradient_clip_norm: 1.0         # æ¢¯åº¦è£å‰ª
```

**Losså‡½æ•°æƒé‡ï¼š**
```yaml
  cosine_loss_weight: 0.5         # Cosineç›¸ä¼¼åº¦Lossçš„æƒé‡
```

**å…¶ä»–å‚æ•°ï¼š**
```yaml
  num_workers: 0                  # DataLoaderçº¿ç¨‹æ•° (CPUè¶³å¤Ÿæ—¶å¯æ”¹>0)
  lazy_load: true                 # æ˜¯å¦å»¶è¿ŸåŠ è½½æ•°æ® (æ¨ètrue)
  checkpoint_interval: 5          # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
```

---

## ğŸš€ å¿«é€Ÿè°ƒå‚æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šæˆ‘æœ‰GPU (æ¨è)
```yaml
training:
  device: "cuda"          # ä½¿ç”¨GPU
  batch_size: 16          # å¢å¤§æ‰¹å¤§å° (GPUå¯ä»¥å¤„ç†)
  num_epochs: 100         # æ›´å¤šè½®æ•°ä»¥è·å¾—æ›´å¥½æ•ˆæœ
  num_workers: 4          # å¤šè¿›ç¨‹åŠ è½½æ•°æ®
  learning_rate: 1.0e-4
```

### æ–¹æ¡ˆBï¼šæ²¡æœ‰GPUï¼ŒCPUè®­ç»ƒ
```yaml
training:
  device: "cpu"           # å¼ºåˆ¶ä½¿ç”¨CPU
  batch_size: 2           # å‡å°æ‰¹å¤§å°èŠ‚çœå†…å­˜
  num_epochs: 20          # ç¼©å‡è½®æ•°åŠ å¿«è®­ç»ƒ
  num_workers: 0          # å…³é—­å¤šè¿›ç¨‹
  lazy_load: true         # å¯ç”¨å»¶è¿ŸåŠ è½½
  learning_rate: 5.0e-5   # é™ä½å­¦ä¹ ç‡æ›´ç¨³å®š
```

### æ–¹æ¡ˆCï¼šå†…å­˜ä¸è¶³ (OOM)
```yaml
training:
  batch_size: 2           # å‡å°åˆ°2æˆ–1
  num_workers: 0
  lazy_load: true
  learning_rate: 1.0e-4
```

### æ–¹æ¡ˆDï¼šæƒ³è¦æ›´å¥½çš„ç²¾åº¦ (æ—¶é—´å……è¶³)
```yaml
training:
  num_epochs: 200         # åŠ å€
  batch_size: 32          # å¢å¤§æ‰¹å¤§å° (éœ€è¦è¶³å¤Ÿæ˜¾å­˜)
  learning_rate: 5.0e-5   # é™ä½å­¦ä¹ ç‡
  num_workers: 4
```

---

## ğŸ“Š è®­ç»ƒè¾“å‡ºè¯´æ˜

è¿è¡Œ `python HachimiNetV1/main.py` æ—¶ï¼Œä½ ä¼šçœ‹åˆ°ï¼š

```
60============================================================
ğŸ“Š Training Config
============================================================
Epochs: 50
Batch Size: 4
Learning Rate: 0.0001
Lazy Load: True
============================================================

ğŸ“Œ Device: cuda:0
ğŸ“Œ CUDA Available: True
   GPU: NVIDIA RTX 3090
   VRAM: 24.0 GB

ğŸš€ Training started on cuda:0
   Model Parameters: 123,456
   Trainable Parameters: 100,000

Train Ep 1:  10%|â–ˆâ–ˆâ–ˆâ–ˆ                  | 102/1000 [00:15<02:30, 5.98it/s]
Train Ep 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1000/1000 [02:30<00:00, 6.67it/s, loss=0.524, mse=0.48, cos=0.19]
Epoch 1/50 | Train Loss: 0.5234 | Val Loss: 0.4892

Train Ep 2:  ...
Epoch 2/50 | Train Loss: 0.4856 | Val Loss: 0.4523

...
```

**å…³é”®æŒ‡æ ‡ï¼š**
- `loss`: æ€»æŸå¤±å€¼ï¼ˆåº”è¯¥é€æ¸ä¸‹é™ï¼‰
- `mse`: å›å½’æŸå¤±ï¼ˆé¢„æµ‹æƒé‡çš„å‡æ–¹è¯¯å·®ï¼‰
- `cos`: Cosineç›¸ä¼¼åº¦æŸå¤±

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜ä½ç½®

è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹ä¼šä¿å­˜åˆ°ï¼š
```
./checkpoints/
â”œâ”€â”€ model_ep5.pth
â”œâ”€â”€ model_ep10.pth
â”œâ”€â”€ model_ep15.pth
â””â”€â”€ model_ep50.pth
```

ä¿®æ”¹é—´éš”ï¼š
```yaml
training:
  checkpoint_interval: 5  # æ”¹ä¸ºå…¶ä»–æ•°å­—
```

---

## ğŸ”§ ä»£ç ä¸­çš„å…¶ä»–å‚æ•°

### main.py ä¸­å¯ä»¥ä¿®æ”¹çš„

```python
# æ•°æ®åŠ è½½
train_loader = DataLoader(
    dataset, 
    batch_size=batch_size,      # ä»configè¯»å–
    shuffle=True, 
    num_workers=num_workers,    # ä»configè¯»å–
    pin_memory=(device.type == 'cuda')
)

# Losså‡½æ•°
loss_fn = WeightsRegressionLoss(
    lambda_cos=train_cfg.get('cosine_loss_weight', 0.5)  # ä»configè¯»å–
)
```

### training/trainer.py ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦

ç›®å‰æ²¡æœ‰å­¦ä¹ ç‡è¡°å‡ã€‚å¦‚æœæƒ³æ·»åŠ ï¼Œå¯ä»¥åœ¨ `_update_scheduler` æ–¹æ³•ä¸­å®ç°ã€‚

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒè¿›åº¦

**æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸‹é™ï¼š**
- Lossåº”è¯¥ä»é«˜å€¼é€æ¸é™ä½
- å¦‚æœlossä¸åŠ¨æˆ–å¢åŠ ï¼Œè¯´æ˜å­¦ä¹ ç‡å¯èƒ½å¤ªé«˜

**æ£€æŸ¥è¿‡æ‹Ÿåˆï¼š**
- å¦‚æœ Val Loss æŒç»­å¢åŠ è€Œ Train Loss ç»§ç»­ä¸‹é™ï¼Œè¯´æ˜è¿‡æ‹Ÿåˆ
- å¯ä»¥å¢åŠ  weight_decay æˆ–å‡å°‘ num_epochs

**æ£€æŸ¥GPUä½¿ç”¨ï¼š**
```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
nvidia-smi
```

---

## ğŸ†˜ æ•…éšœæ’æŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|-------|--------|
| è®­ç»ƒå¾ˆæ…¢ | CPUè®­ç»ƒ | æ£€æŸ¥deviceè®¾ç½®ï¼Œå®‰è£…CUDA |
| OOMé”™è¯¯ | æ˜¾å­˜ä¸è¶³ | å‡å°batch_size |
| Lossä¸ä¸‹é™ | å­¦ä¹ ç‡å¤ªé«˜ | é™ä½learning_rate (10å€) |
| Loss=NaN | æ¢¯åº¦çˆ†ç‚¸ | é™ä½learning_rateæˆ–å¢åŠ gradient_clip_norm |
| ç²¾åº¦ä¸å¥½ | è®­ç»ƒä¸è¶³ | å¢åŠ num_epochsæˆ–å‡å°learning_rate |

---

## ğŸ“ å®Œæ•´é…ç½®ç¤ºä¾‹

**æœ€å°åŒ–é…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰ï¼š**
```yaml
training:
  device: "auto"
  num_epochs: 5
  batch_size: 2
  learning_rate: 1.0e-4
```

**å®Œæ•´é«˜æ•ˆé…ç½®ï¼š**
```yaml
training:
  device: "auto"
  num_epochs: 100
  batch_size: 16
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  optimizer: "AdamW"
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  gradient_clip_norm: 1.0
  regression_loss_weight: 1.0
  cosine_loss_weight: 0.5
  num_workers: 4
  lazy_load: true
  checkpoint_interval: 5
```

---

## æ›´å¤šä¿¡æ¯

è¯¦ç»†çš„è®­ç»ƒé…ç½®æŒ‡å—è§ï¼š`TRAINING_CONFIG_GUIDE.md`
